{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando base no Pyspark, ainda nao sei quais bibliotecas são necessárias instalar porque Estou usando databricks Community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv') \\\n",
    "    .option(\"inferSchema\", 'false') \\\n",
    "    .option('header', 'true') \\\n",
    "    .option('delimiter', ';') \\\n",
    "    .load('/FileStore/tables/dados_acidentes_prf_2023.csv')\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando a tabela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_tabela_permanente = 'teste_salvar'\n",
    "df.write.format('parquet').saveAsTable(nome_tabela_permanente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando com Load\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Importa todo tipo de dados, aqui em especificio estou importando um CSV\n",
    "df = spark.read.load('/FileStore/tables/dados_acidentes_prf_2023.csv', header=True, inferSchema=True, format='csv', sep=';')\n",
    "\n",
    "display(df)\n",
    "\n",
    "# Faz a leitura do tipo do Schema, semelhante ao Dtypes do Pandas\n",
    "display(df.schema)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
