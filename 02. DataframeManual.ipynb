{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando dataframe\n",
    "dataframe = spark.createDataFrame([('Jose', 20), ('maria', 19)])\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o Schema e inserindo dados\n",
    "schema = \"ID INT, nome STRING\"\n",
    "dados = [(20, 'Joao'), (19, 'Maria')]\n",
    "df2 = spark.createDataFrame(dados, schema)\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Somando e agrupando valores \n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "schema =  \"valor INT, nomeProduto STRING\"\n",
    "dados = [ (5, 'Caneta'), (1, 'Borracha'), (4, 'Caneta')  ]\n",
    "\n",
    "df3 = spark.createDataFrame(dados, schema)\n",
    "\n",
    "df_agrupado = df3.groupby('nomeProduto').agg(sum('valor'))\n",
    "display(df_agrupado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Projetando apenas os dados que quero mostrar, assim como no SQL\n",
    "df3.select([\"nomeProduto\", \"valor\"]).show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
