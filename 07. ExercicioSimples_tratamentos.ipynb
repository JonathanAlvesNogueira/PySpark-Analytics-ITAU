{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "import pyspark.sql.functions\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FinancialTransactions\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [  \n",
    "    (\"1\", \"15/01/2023\", \"1000.50\", \"Compra de Ações\"),\n",
    "    (\"2\", \"20/02/2024\", \"1500.75\", \"Compra de Ações\"),\n",
    "    (\"3\", \"17/03/2021\", \"500.00\", \"Venda de Ações\"),\n",
    "    (\"1\", \"02/01/2022\", \"1000.50\", \"Venda de Ações\"),  # Duplicado\n",
    "    (\"4\", \"18/02/2023\", \"2500.25\", \"Compra de Ações\")\n",
    "]\n",
    "\n",
    "schema = (\"id STRING, data STRING, valor STRING, desc STRING\")\n",
    "\n",
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma a coluna data em uma coluna do tipo Data, invés do tipo STRING\n",
    "df= df.withColumn('nova_coluna', to_date( col('data'), 'dd/MM/yyyy'))\n",
    "\n",
    "# Transforma a coluna valor em inteiro\n",
    "df = df.withColumn('valor_int', col('valor').cast('int') )\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# defini um rating pelo valor gasto do cliente\n",
    "condicional_1 = (col('valor_int') >= 1500) \n",
    "condicional_2 = (col('valor_int') <= 1000) \n",
    "\n",
    "df_1 = df.withColumn(\n",
    "    \"Rating por valor\",\n",
    "    when(condicional_1, \"A\")\n",
    "    .when(condicional_2, \"B\")\n",
    "    .otherwise(\"C\")\n",
    ")\n",
    "\n",
    "# Cria a mesma coluna, mas agora deixando apenas uma letra para classificar\n",
    "condicional_1 = col('desc') == 'Compra de Ações'\n",
    "condicional_2 = col('desc') == 'Venda de Ações'\n",
    "\n",
    "df_2 = df_1.withColumn(\n",
    "    'desc'\n",
    "    ,when( condicional_1, 'C')\n",
    "    .otherwise('V')\n",
    ")\n",
    "display(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Uma segunda forma de cria uma coluna, usando CASE WHEN DO SELECT\n",
    "df3 = df2.selectExpr(\n",
    "    '*',\n",
    "    \"(CASE WHEN DESC == 'C' THEN 'COMPROU' \" +\n",
    "    \"ELSE 'VENDEU' END) AS TESTE_TROCA\"\n",
    ")\n",
    "df3.show()\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "# Definir uma UDF para atribuir um rating aleatório, semelhante ao APPLy do pandas\n",
    "def ratingColuna():\n",
    "    return np.random.randint(0, 10)\n",
    "\n",
    "rating = udf(ratingColuna, IntegerType())\n",
    "\n",
    "# Adicionar uma nova coluna de rating ao DataFrame\n",
    "df_with_rating = df.withColumn('rating', rating())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
